
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html manifest="welcome.manifest">
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta content="yes" name="apple-mobile-web-app-capable">
  <meta content="black" name="apple-mobile-web-app-status-bar-style">
  <meta content="telephone=no" name="format-detection">
  <meta name="author" content="Junyi Li">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #07889b; /*#1772d0;*/
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #e37222; /*#f7b733;*/ /*f09228;*/
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    }
    strong {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    }
    heading {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 22px;
    color: #e37222; /*#fc4a1a;*/
    }
    heading2 {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 18px;
    }
    papertitle {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    font-weight: 700;
    }
    name {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 42px;
    }
    li:not(:last-child) {
        margin-bottom: 5px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link href="https://tuchuang-1258543525.cos.ap-beijing.myqcloud.com/20180614_161325021_iOS.jpg" rel="Shortcut Icon" type="image/x-icon">
  <title>Junyi Li (李军毅)</title>

  <link href="./stylesheets/main.css" rel="stylesheet" type="text/css">
  <style id="dark-reader-style" type="text/css">@media screen {

/* Leading rule */
/*html {
  -webkit-filter: brightness(100%) contrast(100%) grayscale(20%) sepia(10%) !important;
}*/

/* Text contrast */
html {
  text-shadow: 0 0 0 !important;
}

/* Full screen */
*:-webkit-full-screen, *:-webkit-full-screen * {
  -webkit-filter: none !important;
}

/* Page background */
html {
  background: rgb(255,255,255) !important;
}

}</style>

<script type="text/javascript">
   function visibility_on(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'none')
            e.style.display = 'block';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'none')
            e.style.display = 'block';
   }
   function visibility_off(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'block')
            e.style.display = 'none';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'block')
            e.style.display = 'none';
   }
   function toggle_visibility(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
   }
   function toggle_vis(id) {
       var e = document.getElementById(id);
       if (e.style.display == 'none')
           e.style.display = 'inline';
       else
           e.style.display = 'none';
   }
</script>

</head>
</div>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="67%" valign="middle">
        <p align="center">
        <name>Junyi Li (李军毅)</name><br>
        </p>
        <p style="text-align:justify">I am a Ph.D. student in <a href="http://ai.ruc.edu.cn/">Gaoling School of Artificial Intelligence</a> at 
		<a href="https://ruc.edu.cn">Renmin University of China</a>, advised by <a href="http://playbigdata.ruc.edu.cn/batmanfly/">Prof. Wayne Xin Zhao</a>. My major research interests lies in the area of Natural Language Generation on large social media. I got my Bachelor's Degree in <a href="https://ruc.edu.cn">Renmin University of China</a> in June 2018, supervised by <a href="http://playbigdata.ruc.edu.cn/batmanfly/">Prof. Wayne Xin Zhao</a>. 
        </p>
        <p>
        <i>There is no authority in science. No one can tell whether your research matters or not. Or how much it matters. All you can do is to contribute to human knowledge and hope it will matter. Even if it doesn't, it does. It eliminates an idea.</i><br>
        </p>
		<p align="right">
		<i>-- Rich Sutton</i>
		</p>


        <p align="center">
			<a href="_files/cv.pdf">CV</a> &nbsp;/&nbsp;
			<a href="https://scholar.google.com/citations?user=zeWrn-4AAAAJ&hl=zh-CN">Google Scholar</a> &nbsp;/&nbsp;
			<a href="https://github.com/turboLJY"> GitHub </a> &nbsp;/&nbsp;
		        <a href="https://www.facebook.com/profile.php?id=100037666562509">Facebook</a> 
        </p>
        </td>
        <td width="33%">
        <img src="./images/lijunyi.jpg" width="95%">
        </td>
      </tr>
  </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>

        <tr><td>
            <heading>Blog Posts</heading>
            <ul>
              <li> <!--<a href="http://bair.berkeley.edu/blog/2018/11/30/visual-rl/">Visual Model-Based Reinforcement Learning as a Path towards Generalist Robots</a>: our work on learning a single model that can be used to accomplish many different tasks, without human supervision.--> </li>
              <li> <a href="javascript:toggle_vis('blogs')">show more</a> </li>
				  <!--<div id="blogs" style="display:none"> 
				  <li> <a href="https://bair.berkeley.edu/blog/2019/03/21/tactile/">Manipulation By Feel</a>: how we can use visual foresight to enable robots to manipulate objects entirely by feel without vision.</li>
				  <li> <a href="http://bair.berkeley.edu/blog/2018/06/28/daml/">One-Shot Imitation from Watching Videos</a>: describes our work on enabling robots to learn to manipulate new objects by watching humans.</li>
				  <li> <a href="http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/">Learning to Learn</a>: covers recent approaches to meta-learning and our recent paper on model-agnostic meta-learning. It was also <a href="https://www.jiqizhixin.com/articles/47f6d636-281d-4cbe-a0cc-9d3670fcdc64">translated into Chinese</a>.</li>
				  <li><a href="https://research.googleblog.com/2016/10/how-robots-can-acquire-new-skills-from.html">How Robots Can Acquire New Skills from Their Shared Experience</a>:
				  features some work done by me and my colleagues at Google Brain, X, and DeepMind on learning across multiple robots. It was nicely summarized
				  by the MIT Technology Review <a href="https://www.technologyreview.com/s/602529/google-builds-a-robotic-hive-mind-kindergarten/">here</a>. </li>
				 </div>-->
            </ul>
        </td></tr>

        <tr><td width="100%" valign="middle">
            <heading>Publications</heading> <br><br>

            <heading2><i>2019</i></heading2><br><br>

            <div onmouseover="document.getElementById('ACF').style.display = 'block';"
                onmouseout="document.getElementById('ACF').style.display='none';">
              <a href="https://www.aclweb.org/anthology/P19-1190.pdf">
                <papertitle>Generating Long and Informative Reviews with Aspect-Aware Coarse-to-Fine Decoding</papertitle></a><br>
              <i><strong>Junyi Li</strong></i>,
              <i>Wayne Xin Zhao</i>,
			  <i>Ji-Rong Wen</i>,
			  <i>Yang Song</i>
              <br>
              <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2019 <br>
              <a href="https://www.aclweb.org/anthology/P19-1190.pdf">pdf</a>  /  <a href="https://github.com/turboLJY/Coarse-to-Fine-Review-Generation">code</a> 
            </div>
              <div id="ACF" style="display:none;text-align:justify">
              Generating long and informative review text
				is a challenging natural language generation
				task. Previous work focuses on word-level
				generation, neglecting the importance of topical and syntactic characteristics from natural
				languages. In this paper, we propose a novel
				review generation model by characterizing an
				elaborately designed aspect-aware coarse-tofine generation process. First, we model the
				aspect transitions to capture the overall content
				flow. Then, to generate a sentence, an aspectaware sketch will be predicted using an aspectaware decoder. Finally, another decoder fills in
				the semantic slots by generating corresponding words. Our approach is able to jointly
				utilize aspect semantics, syntactic sketch, and
				context information. Extensive experiments
				results have demonstrated the effectiveness of
				the proposed model.
              </div><br>

            
        </td></tr>
		
		<tr><td width="100%" valign="middle">
            <heading>Awards</heading> <br><br>
			<ul>
				<li>National Scholarship for Master student, 2019.</li>
				<li>National Mathematical Modeling Contest.</li>
             </div>
            </ul>
			
		</td></tr>
		
		<tr><td width="100%" valign="middle">
            <heading>Contact</heading> <br><br>
			
			<p>Gaoling School of Artificial Intelligence</p>
			<p>Renmin University of China</p>
			<p>Email: lijunyi at ruc dot edu dot cn</p>
			<p>Address: Information Building 125, No. 59 Zhongguancun Street, Haidian District Beijing, P.R. China</p>
			
		</td></tr>

      </tbody></table>

  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-59618557-1', 'auto');
                ga('send', 'pageview');

              </script>

    </td>
    </tr>
  </tbody></table>
  
  <!--footer start-->
 <footer class="footer">
	<p>Copyright 2019. All Rights Reserved by Junyi Li.</p>
 </footer>
     <!--footer end-->
  
</body></html>
